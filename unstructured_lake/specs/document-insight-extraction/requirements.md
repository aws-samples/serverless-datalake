# Requirements Document

## Introduction

The Document Insight Extraction System is a serverless AWS CDK-based application that enables users to upload PDF documents, automatically process and extract text and images, generate vector embeddings, and extract structured insights through natural language queries. The system leverages AWS services including S3, Lambda, API Gateway, DynamoDB, AppRunner, and Amazon Bedrock to provide a scalable, cost-effective solution for document analysis and insight extraction.

## Glossary

- **System**: The Document Insight Extraction System
- **User**: An authenticated individual interacting with the web interface
- **PDF Document**: A Portable Document Format file uploaded by the User
- **Document Page**: A single page within a PDF Document
- **Text Chunk**: A segment of extracted text with a maximum of 8192 tokens and 10% overlap with adjacent chunks
- **Vector Embedding**: A numerical representation of a Text Chunk generated by Amazon Titan V2 embedding model
- **S3 Vector Store**: Amazon S3-based storage for Vector Embeddings with associated metadata
- **Insight**: Structured information extracted from a PDF Document in JSON format
- **Presigned POST URL**: A time-limited URL that allows direct upload to S3 without AWS credentials
- **WebSocket Connection**: A bidirectional communication channel between the UI and backend for real-time updates
- **Ingestion Process**: The complete workflow of uploading, processing, chunking, embedding, and storing a PDF Document
- **Cache**: DynamoDB table storing recently extracted Insights with a 24-hour TTL
- **OCR**: Optical Character Recognition performed by Amazon Bedrock on images within Document Pages
- **API Gateway**: AWS service providing REST and WebSocket endpoints for the System
- **Lambda Function**: Serverless compute function processing documents and queries
- **AppRunner Service**: AWS service hosting the React-based frontend application
- **CDK Stack**: Infrastructure as Code definition for AWS resources

## Requirements

### Requirement 1

**User Story:** As a User, I want to upload PDF documents through a web interface, so that I can extract insights from my documents without managing infrastructure.

#### Acceptance Criteria

1. WHEN the User requests an upload URL, THE System SHALL generate a Presigned POST URL valid for 15 minutes
2. THE System SHALL create an S3 bucket with event notifications configured to trigger the Lambda Function upon object creation
3. WHEN a PDF Document is uploaded to S3, THE System SHALL trigger the Ingestion Process within 5 seconds
4. THE System SHALL support PDF Documents up to 100 MB in size
5. THE System SHALL validate that uploaded files are valid PDF format before processing

### Requirement 2

**User Story:** As a User, I want to see real-time progress of document ingestion, so that I know when my document is ready for querying.

#### Acceptance Criteria

1. THE System SHALL establish a WebSocket Connection through API Gateway for real-time communication
2. WHEN the Ingestion Process begins, THE System SHALL send a status message "processing_started" through the WebSocket Connection
3. WHILE the Ingestion Process is active, THE System SHALL send progress updates every 10 pages processed
4. WHEN the Ingestion Process completes successfully, THE System SHALL send a status message "processing_complete" with the document identifier
5. IF the Ingestion Process fails, THEN THE System SHALL send an error message with failure reason through the WebSocket Connection

### Requirement 3

**User Story:** As a User, I want the system to extract text and images from PDF pages, so that all content is available for analysis.

#### Acceptance Criteria

1. THE System SHALL use pypdf library to extract text from each Document Page
2. WHEN a Document Page contains images, THE System SHALL invoke Amazon Bedrock to perform OCR on those images
3. THE System SHALL combine extracted text and OCR results into a single text representation per Document Page
4. THE System SHALL preserve the page number as metadata for each extracted text segment
5. THE System SHALL handle Document Pages with no text content by relying solely on OCR results

### Requirement 4

**User Story:** As a User, I want documents to be split into optimal chunks for embedding, so that retrieval accuracy is maximized.

#### Acceptance Criteria

1. WHEN the System has processed more than 10 Document Pages, THE System SHALL split the accumulated text into Text Chunks
2. THE System SHALL use a recursive character text splitter with 8192 token maximum per Text Chunk
3. THE System SHALL configure 10% overlap between consecutive Text Chunks
4. THE System SHALL preserve document identifier and page range as metadata for each Text Chunk
5. THE System SHALL continue chunking for every additional 10 pages processed until the PDF Document is complete

### Requirement 5

**User Story:** As a User, I want document chunks to be converted to vector embeddings, so that semantic search can be performed.

#### Acceptance Criteria

1. THE System SHALL invoke Amazon Titan V2 embedding model for each Text Chunk
2. THE System SHALL provide Text Chunks with maximum 8192 input tokens to the embedding model
3. THE System SHALL receive 512-dimensional Vector Embeddings from the embedding model
4. THE System SHALL store Vector Embeddings in the S3 Vector Store with associated metadata
5. THE System SHALL include document identifier, page range, and upload timestamp in the metadata

### Requirement 6

**User Story:** As a User, I want to filter vector search by specific documents, so that I can extract insights from individual documents rather than all documents.

#### Acceptance Criteria

1. THE System SHALL support metadata filtering in the S3 Vector Store by document identifier
2. WHEN the User selects a specific PDF Document, THE System SHALL query only Vector Embeddings with matching document identifier
3. THE System SHALL verify metadata filtering capability during deployment and log confirmation
4. THE System SHALL return an error if metadata filtering is not supported by the S3 Vector Store implementation
5. THE System SHALL retrieve the top 5 most relevant Text Chunks based on semantic similarity

### Requirement 7

**User Story:** As a User, I want to extract structured insights from documents using natural language prompts, so that I can quickly obtain specific information.

#### Acceptance Criteria

1. THE System SHALL accept a natural language prompt and document identifier from the User through API Gateway
2. THE System SHALL query the S3 Vector Store with metadata filtering for the specified document identifier
3. THE System SHALL use retrieved Text Chunks as context for Amazon Bedrock to generate structured Insights
4. THE System SHALL format extracted Insights as valid JSON with consistent schema
5. THE System SHALL return Insights to the User within 30 seconds of query submission

### Requirement 8

**User Story:** As a User, I want frequently accessed insights to be cached, so that repeated queries are faster and more cost-effective.

#### Acceptance Criteria

1. THE System SHALL create a DynamoDB table with document identifier as partition key and extraction timestamp as sort key
2. THE System SHALL configure a TTL attribute on DynamoDB records set to 24 hours from creation
3. WHEN the User queries for Insights, THE System SHALL first check the Cache for existing results
4. IF valid cached Insights exist, THEN THE System SHALL return cached results without querying the S3 Vector Store
5. IF cached Insights do not exist or have expired, THEN THE System SHALL query the S3 Vector Store, store results in the Cache, and return the Insights

### Requirement 9

**User Story:** As a User, I want to retrieve previously extracted insights, so that I can review historical analysis without re-processing.

#### Acceptance Criteria

1. THE System SHALL provide a GET API endpoint to retrieve Insights by document identifier
2. THE System SHALL query the DynamoDB table using the document identifier as partition key
3. THE System SHALL return all non-expired Insight records sorted by extraction timestamp in descending order
4. THE System SHALL return an empty result set if no cached Insights exist for the document identifier
5. THE System SHALL include extraction timestamp and TTL expiration time in the response

### Requirement 10

**User Story:** As a Developer, I want all infrastructure defined as CDK code, so that the system can be deployed consistently across environments.

#### Acceptance Criteria

1. THE System SHALL define all AWS resources using AWS CDK with Python
2. THE System SHALL create separate CDK Stack modules for S3, Lambda, API Gateway, DynamoDB, and AppRunner resources
3. THE System SHALL configure S3 event notifications to trigger the Lambda Function on object creation and deletion
4. THE System SHALL create Lambda layers for pypdf and other dependencies following the existing project pattern
5. THE System SHALL output API Gateway endpoints, WebSocket URLs, and AppRunner service URLs as CDK outputs

### Requirement 11

**User Story:** As a User, I want to interact with the system through a modern web interface, so that document management and insight extraction are intuitive.

#### Acceptance Criteria

1. THE System SHALL deploy a React-based frontend using AWS Cloudscape Design System components
2. THE System SHALL host the frontend on an AppRunner Service with automatic scaling
3. THE System SHALL provide UI components for document upload, document selection, and prompt input
4. THE System SHALL display real-time ingestion progress using WebSocket updates
5. THE System SHALL render extracted Insights in a structured, readable format

### Requirement 12

**User Story:** As a User, I want my documents and insights to be automatically cleaned up, so that storage costs are minimized.

#### Acceptance Criteria

1. THE System SHALL automatically delete DynamoDB Cache records after 24 hours using TTL
2. THE System SHALL remove Vector Embeddings from S3 Vector Store when the source PDF Document is deleted from S3
3. WHEN an S3 delete event is received, THE System SHALL identify and remove all associated Vector Embeddings by document identifier
4. THE System SHALL log all cleanup operations for audit purposes
5. THE System SHALL complete cleanup operations within 60 seconds of the triggering event
